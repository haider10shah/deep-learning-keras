{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## The Vanishing and Exploding Gradients Problems\n",
    "We need the variance of the output of the layers to be equal to the variance of the inputs\n",
    "\n",
    "Sigmoid is an example of saturating activation function\n",
    "\n",
    "Glorot Initialization\n",
    "fan_avg = (fan_in + fan_out)/2\n",
    "\n",
    "RELU suffers from a problem called dying RELU's. This is when the weighted sum of outputs for a nueron goes negative and it effectively outputs 0\n",
    "To counter this you can use Leaky RELU\n",
    "\n",
    "ELU or exponential linear unit outperfoms RELU\n",
    "It is slower to compute\n",
    "\n",
    "Scaled ELU gurantess self-normalization under certain conditions\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Batch Normalization\n",
    "This operation is added just before or after the activation of each layer.\n",
    "This operation zero-centers and normalizes each input\n",
    "There are two new parameter vectors, one for scaling and one for shifting\n",
    "\n",
    "\n",
    "It calculates the mean and sd over each mini Batch\n",
    "lambda the scale vector and beta the offset vector are learned through back prop\n",
    "\n",
    "Batch Normalization also acts as regularizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    # This gives the same affect as standard scaler\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "source": [
    "## Gradient Clipping\n",
    "This is to clip the gradients in the backprop to some max threshold\n",
    "This is often used for RNN as Batch Norm is tricky to use in RNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "# Another option is to use clip norm"
   ]
  },
  {
   "source": [
    "## Transfer Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A is created on 8 classes of Fashion Mnist data set\n",
    "# Want to train a binary classifier which is model B\n",
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "# Take all but the last layer\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Training model B will also affect the weights of Model A\n",
    "# You will need to clone Model A using clone.model() and then copy its weights\n",
    "\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "\n",
    "# Initially there will be large errors in the last layer that might get propogated to the to first layers\n",
    "# We might want to freeze those layers for first few epochs\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer='sgd', metrics=[\"accuracy\"])\n",
    "# You must always compile the model after you freeze or unfreeze\n",
    "\n",
    "# After few epochs, you can unfreeze the model, recompile the model and probably lower the learning rate to avoid \n",
    "# damaging the learned rates for earlier layers\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=1e-4)\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "# transfer learning does not work very well with small dense networks. small patterns learn very few patterns\n",
    "# and dense networks learn very specific patterns\n",
    "\n"
   ]
  }
 ]
}